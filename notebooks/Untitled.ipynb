{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspected-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "otherwise-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier, \\\n",
    "    RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "characteristic-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will be used to split the samples ID-wise. \n",
    "# This function is only called once before the main loop for training with cross-validation.\n",
    "def split_train_test(labels_df):\n",
    "    from sklearn.model_selection import StratifiedKFold   \n",
    "    # First, get all unique samples and their category\n",
    "    unique_samples = []\n",
    "    unique_cats = []\n",
    "    for sample, cat in zip(labels_df['sample'], labels_df['category']):\n",
    "        if sample not in unique_samples:\n",
    "            unique_samples += [sample]\n",
    "            unique_cats += [cat]\n",
    "\n",
    "    # StratifiedKFold with n_splits of 5 to ranmdomly split 80/20.\n",
    "    # Used only once for train/test split.\n",
    "    # The train split needs to be split again into train/valid sets later\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    train_inds, test_inds = next(skf.split(unique_samples, unique_cats))\n",
    "    \n",
    "    # After the samples are split, we get the duplicates of all samples.\n",
    "    train_samples = [unique_samples[s] for s in train_inds]\n",
    "    test_samples = [unique_samples[s] for s in test_inds]\n",
    "    train_cats = [unique_cats[ind] for ind in train_inds]\n",
    "\n",
    "    return train_samples, test_samples, train_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggressive-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sgd = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    'penalty': ['elasticnet'],\n",
    "    'l1_ratio': [0, 0.15, 0.5, 0.85, 1.0],\n",
    "    'max_iter': [10000],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'max_depth': [300],\n",
    "    'max_features': [100],\n",
    "    'min_samples_split': [300],\n",
    "    'n_estimators': [100],\n",
    "    'criterion': ['entropy'],\n",
    "    'min_samples_leaf': [3],\n",
    "    'oob_score': [False],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "param_grid_rfr = {\n",
    "    'max_depth': [300],\n",
    "    'max_features': [100],\n",
    "    'min_samples_split': [300],\n",
    "    'n_estimators': [100],\n",
    "    'min_samples_leaf': [3],\n",
    "    'oob_score': [False],\n",
    "}\n",
    "param_grid_lda = {\n",
    "}\n",
    "param_grid_qda = {\n",
    "}\n",
    "param_grid_logreg = {\n",
    "    # 'max_iter': [10000],\n",
    "    'solver': ['saga'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "param_grid_linsvc = {\n",
    "    'max_iter': [10000],\n",
    "    'C': [1]\n",
    "}\n",
    "param_grid_svc = {\n",
    "    'max_iter': [10000],\n",
    "    'C': [1],\n",
    "    'kernel': ['linear'],\n",
    "    'probability': [True]\n",
    "}\n",
    "param_grid_ada = {\n",
    "    'base_estimator': [LinearSVC(max_iter=10000)],\n",
    "    'learning_rate': (1)\n",
    "}\n",
    "param_grid_bag = {\n",
    "    'base_estimator': [\n",
    "        LinearSVC(max_iter=10000)],\n",
    "    'n_estimators': [10]\n",
    "}\n",
    "\n",
    "param_grid_voting = {\n",
    "    'voting': ('soft', 'hard'),\n",
    "}\n",
    "rf = RandomForestClassifier(max_depth=300, max_features=100, min_samples_split=300, n_estimators=100)\n",
    "gnb = GaussianNB()\n",
    "cnb = CategoricalNB()\n",
    "lr = LogisticRegression(max_iter=4000)\n",
    "lsvc = LinearSVC(max_iter=10000)\n",
    "estimators_list = [('rf', rf),\n",
    "                   ('lr', lr),\n",
    "                   ('lsvc', lsvc),\n",
    "                   ('gnb', gnb),\n",
    "                   ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "substantial-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DIR = 'src/models/sklearn/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "novel-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimators_list():\n",
    "    rf = RandomForestClassifier(max_depth=300, max_features=100, min_samples_split=300, n_estimators=100)\n",
    "    gnb = GaussianNB()\n",
    "    lr = LogisticRegression(max_iter=4000)\n",
    "    lsvc = SVC(kernel='linear', probability=True)\n",
    "    estimators_list = [('gnb', gnb),\n",
    "                       ('lr', lr),\n",
    "                       ('lsvc', lsvc)\n",
    "                       ]\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "declared-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the data from a csv file.\n",
    "def ms_data(fname):\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    mat_data = pd.read_csv(fname)\n",
    "    labels = mat_data.index.values\n",
    "    categories = [int(lab.split('_')[1]) for lab in labels]\n",
    "    labels = [lab.split('_')[0] for lab in labels]\n",
    "    mat_data = np.asarray(mat_data)\n",
    "    mat_data = minmax_scale(mat_data, axis=0, feature_range=(0, 1))\n",
    "    mat_data = mat_data.astype(\"float32\")\n",
    "    return mat_data, labels, categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stainless-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogisticRegression\": [LogisticRegression, param_grid_logreg],\n",
    "    \"BaggingClassifier\": [BaggingClassifier, param_grid_bag],\n",
    "    \"LinearSVC\": [LinearSVC, param_grid_linsvc],\n",
    "    \"SVCLinear\": [SVC, param_grid_svc],\n",
    "    \"Gaussian_Naive_Bayes\": [GaussianNB, {}],\n",
    "    \"SGDClassifier\": [SGDClassifier, params_sgd],\n",
    "    \"KNeighbors\": [KNeighborsClassifier, {}],\n",
    "    # \"AdaBoost_Classifier\": [AdaBoostClassifier, param_grid_ada],\n",
    "    \"LDA\": [LinearDiscriminantAnalysis, param_grid_lda],\n",
    "    \"QDA\": [QuadraticDiscriminantAnalysis, param_grid_qda],\n",
    "    \"RandomForestClassifier\": [RandomForestClassifier, param_grid_rf],\n",
    "    # \"Voting_Classifier\": [VotingClassifier, param_grid_voting],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bigger-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities_csv = 'data/canis_intensities.csv'\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afraid-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, samples = ms_data('data/canis_intensities.csv')\n",
    "data[np.isnan(data)] = 0\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if label != 'Normal':\n",
    "        labels[i] = 'Not Normal'\n",
    "\n",
    "nb_classes = len(set(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scheduled-telescope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2228 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample  category       label\n",
       "0          1         1  Not Normal\n",
       "1          1         1  Not Normal\n",
       "2          1         1  Not Normal\n",
       "3          1         1  Not Normal\n",
       "4          1         1  Not Normal\n",
       "...      ...       ...         ...\n",
       "2223       9         1  Not Normal\n",
       "2224       9         1  Not Normal\n",
       "2225       9         1  Not Normal\n",
       "2226       9         1  Not Normal\n",
       "2227       9         1  Not Normal\n",
       "\n",
       "[2228 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = pd.Categorical(labels).codes\n",
    "labels_df = pd.concat([\n",
    "    pd.DataFrame(np.array(samples).reshape([-1, 1])),\n",
    "    pd.DataFrame(np.array(categories).reshape([-1, 1])),\n",
    "    pd.DataFrame(np.array(labels).reshape([-1, 1])),\n",
    "], 1)\n",
    "labels_df.columns = ['sample', 'category', 'label']\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "based-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_samples, test_samples, train_cats = split_train_test(labels_df)\n",
    "all_train_indices = [s for s, lab in enumerate(labels_df['sample']) if lab in all_train_samples]\n",
    "test_indices = [s for s, lab in enumerate(labels_df['sample']) if lab in test_samples]\n",
    "\n",
    "x_test = data[test_indices]\n",
    "y_test = labels_df['category'][test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-insulin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "delayed-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_params: {'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.8504074505238649, Best Valid score: 0.8504074505238649\n",
      "Best model\n",
      "Train score: 0.8482211362053901 +- 0.05130741558593411\n",
      "Valid score: 0.7978201321949293 +- 0.030475901216540296\n",
      "Test score: 0.703921568627451\n",
      "\n",
      "h_params: {'base_estimator': LinearSVC(max_iter=10000), 'n_estimators': 10}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.9941792782305006, Best Valid score: 0.9941792782305006\n",
      "Best model\n",
      "Train score: 0.9805409673581308 +- 0.002356648600947422\n",
      "Valid score: 0.8473026364935139 +- 0.04229599591201576\n",
      "Test score: 0.8254901960784313\n",
      "\n",
      "h_params: {'C': 1, 'max_iter': 10000}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.9965075669383003, Best Valid score: 0.9965075669383003\n",
      "Best model\n",
      "Train score: 1.0 +- 0.0\n",
      "Valid score: 0.8628514325925133 +- 0.035845329366991315\n",
      "Test score: 0.8294117647058824\n",
      "\n",
      "h_params: {'C': 1, 'kernel': 'linear', 'max_iter': 10000, 'probability': True}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.9883585564610011, Best Valid score: 0.9883585564610011\n",
      "Best model\n",
      "Train score: 1.0 +- 0.0\n",
      "Valid score: 0.9278478512445285 +- 0.04127363902336969\n",
      "Test score: 0.8019607843137255\n",
      "\n",
      "h_params: {}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.7561117578579744, Best Valid score: 0.7561117578579744\n",
      "Best model\n",
      "Train score: 0.7693482370493402 +- 0.021548519272859373\n",
      "Valid score: 0.7379550896125039 +- 0.030911437439226187\n",
      "Test score: 0.6823529411764706\n",
      "\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0, 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0, 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0, 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0, 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.15, 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.15, 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.15, 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.15, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.15, 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.5, 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.5, 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.5, 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.5, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.5, 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.85, 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.85, 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.85, 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.85, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 0.85, 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 1.0, 'loss': 'hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 1.0, 'loss': 'log', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 1.0, 'loss': 'modified_huber', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 1.0, 'loss': 'squared_hinge', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "h_params: {'class_weight': 'balanced', 'l1_ratio': 1.0, 'loss': 'perceptron', 'max_iter': 10000, 'penalty': 'elasticnet'}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.9965075669383003, Best Valid score: 0.9965075669383003\n",
      "Best model\n",
      "Train score: 0.9944033232050693 +- 0.016510976373563732\n",
      "Valid score: 0.9350843948642518 +- 0.025839728585901715\n",
      "Test score: 0.8490196078431372\n",
      "\n",
      "h_params: {}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.930151338766007, Best Valid score: 0.930151338766007\n",
      "Best model\n",
      "Train score: 0.9226374954649343 +- 0.03158299416625953\n",
      "Valid score: 0.8476766152933148 +- 0.014474717327151157\n",
      "Test score: 0.7352941176470589\n",
      "\n",
      "h_params: {}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.9941792782305006, Best Valid score: 0.9941792782305006\n",
      "Best model\n",
      "Train score: 0.9895729400124861 +- 0.00540876667498211\n",
      "Valid score: 0.7759403417500117 +- 0.016618238003203084\n",
      "Test score: 0.7176470588235294\n",
      "\n",
      "h_params: {}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.9982537834691502, Best Valid score: 0.9982537834691502\n",
      "Best model\n",
      "Train score: 0.995505693280912 +- 0.002972897488310133\n",
      "Valid score: 0.5693512217639102 +- 0.02211799219284295\n",
      "Test score: 0.7019607843137254\n",
      "\n",
      "h_params: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 300, 'max_features': 100, 'min_samples_leaf': 3, 'min_samples_split': 300, 'n_estimators': 100, 'oob_score': False}\n",
      "CV: 0\n",
      "CV: 1\n",
      "CV: 2\n",
      "Best model\n",
      "Train score: 0.9417927823050058, Best Valid score: 0.9417927823050058\n",
      "Best model\n",
      "Train score: 0.8695051809668389 +- 0.04001635810327955\n",
      "Valid score: 0.8403339075383051 +- 0.09532883411119664\n",
      "Test score: 0.8431372549019608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_test = data[test_indices]\n",
    "y_test = labels_df['category'][test_indices].tolist()\n",
    "all_x_train = data[all_train_indices]\n",
    "all_y_train = labels_df['category'][all_train_indices].tolist()\n",
    "\n",
    "estimators_list = get_estimators_list()\n",
    "best_params = {}\n",
    "scaler = StandardScaler()\n",
    "for name, (model, param_grid) in zip(models.keys(), models.values()):\n",
    "    best_scores_train = []\n",
    "    best_scores_valid = []\n",
    "    for g in ParameterGrid(param_grid):\n",
    "        print('h_params:', g)\n",
    "        best_score = np.inf\n",
    "        # 3-fold CV\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        for i, (train_samples, valid_samples) in enumerate(skf.split(all_train_samples, train_cats)):\n",
    "            # Just plot the first iteration, it will already be crowded if doing > 100 optimization iterations\n",
    "            if verbose:\n",
    "                print(f\"CV: {i}\")\n",
    "\n",
    "            # Now valid_samples and train_samples are indices of all_train_samples.\n",
    "            # So we first need to get the correct samples indices\n",
    "            valid_samples = [all_train_samples[s] for s in valid_samples]\n",
    "            train_samples = [all_train_samples[s] for s in train_samples]\n",
    "            \n",
    "            \n",
    "            # Next, we get all the indices for all replicates of each set\n",
    "            train_indices = [s for s, lab in enumerate(labels_df['sample'].tolist()) if lab in train_samples]\n",
    "            valid_indices = [s for s, lab in enumerate(labels_df['sample'].tolist()) if lab in valid_samples]\n",
    "\n",
    "            x_train = data[train_indices]\n",
    "            y_train = labels_df['category'][train_indices]\n",
    "            x_valid = data[valid_indices]\n",
    "            y_valid = labels_df['category'][valid_indices]\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_valid = scaler.transform(x_valid)\n",
    "\n",
    "            dir_name = f\"saved_models/sklearn/\"\n",
    "            os.makedirs(dir_name, exist_ok=True)\n",
    "            pickle.dump(scaler, open(f\"{dir_name}/scaler.sav\", 'wb'))\n",
    "\n",
    "            m = model()\n",
    "            m.set_params(**g)\n",
    "            m = m.fit(x_train, y_train)\n",
    "            valid_score = m.score(x_valid, y_valid)\n",
    "            score_train = m.score(x_train, y_train)\n",
    "            score_valid = m.score(x_valid, y_valid)\n",
    "            best_scores_train += [score_train]\n",
    "            best_scores_valid += [score_valid]\n",
    "\n",
    "        # save if best\n",
    "        if np.mean(best_scores_valid) < best_score:\n",
    "            best_score = valid_score\n",
    "            best_grid = g\n",
    "    best_params[name] = best_grid\n",
    "    best_model = model()\n",
    "    best_model.set_params(**best_grid)\n",
    "    best_model.fit(all_x_train, all_y_train)\n",
    "    score_train = best_model.score(all_x_train, all_y_train)\n",
    "    score_test = best_model.score(all_x_train, all_y_train)\n",
    "\n",
    "    print(f\"Best model\\n\"\n",
    "          f\"Train score: {score_train}, \"\n",
    "          f\"Best Valid score: {score_test}\"\n",
    "          )\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    filename = f\"{dir_name}/{name}.sav\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "    # TODO find best grid according to all cv iterations\n",
    "    best_model = model()\n",
    "    best_model.set_params(**best_grid)\n",
    "    best_model.fit(X=all_x_train, y=all_y_train)\n",
    "    score_test = best_model.score(x_test, y_test)\n",
    "    best_params[name]['train_acc_mean'] = np.mean(best_scores_train)\n",
    "    best_params[name]['train_acc_std'] = np.std(best_scores_train)\n",
    "    best_params[name]['valid_acc_mean'] = np.mean(best_scores_valid)\n",
    "    best_params[name]['valid_acc_std'] = np.std(best_scores_valid)\n",
    "    best_params[name]['test_acc'] = score_test\n",
    "\n",
    "    print(\n",
    "        f\"Best model\\n\"\n",
    "        f\"Train score: {np.mean(best_scores_train)} +- {np.std(best_scores_train)}\\n\"\n",
    "        f\"Valid score: {np.mean(best_scores_valid)} +- {np.std(best_scores_valid)}\\n\"\n",
    "        f\"Test score: {score_test}\\n\"\n",
    "    )\n",
    "for name in best_params.keys():\n",
    "    for param in best_params[name].keys():\n",
    "        best_params[name][param] = str(best_params[name][param])\n",
    "\n",
    "json.dump(best_params, open('saved_models/sklearn/best_params.json', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = {\n",
    "    \"SVCLinear\": SVC(max_iter=10000, kernel='linear', probability=True),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(max_depth=300, max_features=100, min_samples_split=300,\n",
    "                                                     n_estimators=100, class_weight='balanced',\n",
    "                                                     criterion='entropy'),\n",
    "    \"Bagging_Classifier\":\n",
    "        BaggingClassifier(\n",
    "            base_estimator=LinearSVC(max_iter=4000), n_estimators=100),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=10000, penalty='l2', class_weight='balanced'),\n",
    "    \"LinearSVC\": GridSearchCV(estimator=LinearSVC(max_iter=4000), param_grid={}, n_jobs=-1, cv=5),\n",
    "    # \"Voting_Classifier\": VotingClassifier(estimators=estimators_list, voting='hard')\n",
    "}\n",
    "# final_train(final_models, args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
