{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "toxic-abraham",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This notebook is intended for demonstration purposes only. \n",
    "The code might not be maintained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-iceland",
   "metadata": {},
   "source": [
    "This tutorial uses MS data thast. Canine sarcoma raw library is accessible on the ProteomeXchange consortium: PXD010990\n",
    "<br>\n",
    "The data already preprocessed is available in the file `data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increased-bronze",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "large-lingerie",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import random\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-tournament",
   "metadata": {},
   "source": [
    "Seed all random values generators. This will make the results reproducible, as all randomly chosen variables will always be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-latest",
   "metadata": {},
   "source": [
    "Definition of the models to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "silver-president",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, Dropout, BatchNormalization, MaxPooling1D, LeakyReLU\n",
    "\n",
    "# This is an abstract class to make sure all models used are polymorphic. Does't make much sense now because\n",
    "# we only define a single model (CNN), but it is very useful to add subsequent models to make sure all models \n",
    "# respect the same architecture. \n",
    "# Having the same methods for all models classes will make sure they all can be used in the Train class.\n",
    "# It will help making sure everything is reproducible\n",
    "\n",
    "class Base(Layer):\n",
    "    def __init__(self, h_params, nb_classes, batch_size, variant, activation):\n",
    "        super(Base, self).__init__()\n",
    "\n",
    "        self.model = None\n",
    "        self.__previous_models = None\n",
    "        self.__input_shape = None\n",
    "\n",
    "        self.__h_params = h_params\n",
    "        self.__activation = activation\n",
    "        self.__variant = variant\n",
    "        self.__nb_classes = nb_classes\n",
    "        self.__batch_size = batch_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        exit('The function `build` needs to be implemented. This is an abstract class.')\n",
    "\n",
    "    def get_model_name(self):\n",
    "        exit('`get_model_name` needs to be implemented. This is an abstract class.')\n",
    "\n",
    "    @property\n",
    "    def variant(self):\n",
    "        return self.__variant\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.__batch_size\n",
    "\n",
    "    @property\n",
    "    def nb_classes(self):\n",
    "        return self.__nb_classes\n",
    "\n",
    "    @property\n",
    "    def input_shape(self):\n",
    "        return self.__input_shape\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self.__activation\n",
    "\n",
    "    @property\n",
    "    def h_params(self):\n",
    "        return self.__h_params\n",
    "\n",
    "    @property\n",
    "    def n_epochs(self):\n",
    "        return self.__h_params['n_epochs']\n",
    "\n",
    "    @property\n",
    "    def wd(self):\n",
    "        return self.__h_params['wd']\n",
    "\n",
    "    @property\n",
    "    def l1(self):\n",
    "        return self.__h_params['l1']\n",
    "\n",
    "    @variant.setter\n",
    "    def variant(self, variant):\n",
    "        self.__variant = variant\n",
    "\n",
    "    @batch_size.setter\n",
    "    def batch_size(self, batch_size):\n",
    "        self.__batch_size = batch_size\n",
    "\n",
    "    @nb_classes.setter\n",
    "    def nb_classes(self, nb_classes):\n",
    "        self.__nb_classes = nb_classes\n",
    "\n",
    "    @input_shape.setter\n",
    "    def input_shape(self, input_shape):\n",
    "        self.__input_shape = input_shape\n",
    "\n",
    "    @activation.setter\n",
    "    def activation(self, activation):\n",
    "        self.__activation = activation\n",
    "\n",
    "    @h_params.setter\n",
    "    def h_params(self, h_params):\n",
    "        self.__h_params = h_params\n",
    "\n",
    "    @n_epochs.setter\n",
    "    def n_epochs(self, n_epochs):\n",
    "        self.__h_params['n_epochs'] = n_epochs\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, wd):\n",
    "        self.__h_params['wd'] = wd\n",
    "\n",
    "    @l1.setter\n",
    "    def l1(self, l1):\n",
    "        self.__h_params['l1'] = l1\n",
    "\n",
    "\n",
    "class CNN(Base):\n",
    "    def __init__(self, h_params, nb_classes, batch_size, variant='lecun', activation='relu'):\n",
    "        super(CNN, self).__init__(h_params, nb_classes, batch_size, variant, activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        if self.variant == 'lecun':\n",
    "            self.lecun()\n",
    "        elif self.variant == 'lenet':\n",
    "            self.lenet()\n",
    "        elif self.variant == 'vgg9':\n",
    "            self.vgg9()\n",
    "        else:\n",
    "            exit(f'Model {self.variant} unrecognized.\\n Accepted variants: lecun, lenet and vgg9')\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"CNN\"\n",
    "\n",
    "    def lecun(self):\n",
    "        self.model = Sequential([\n",
    "            Conv1D(filters=6, kernel_size=21, strides=1, padding='same', activation='relu',\n",
    "                   input_shape=self.input_shape, kernel_initializer=keras.initializers.he_normal(), activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Conv1D(filters=16, kernel_size=5, strides=1, padding='same', activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Flatten(),\n",
    "            Dense(120, activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            Dense(84, activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            Dense(self.nb_classes, activation='softmax', activity_regularizer=l1_l2(self.l1, self.wd))  # or Activation('softmax')\n",
    "        ])\n",
    "\n",
    "    def lenet(self):\n",
    "        self.model = Sequential([\n",
    "            Conv1D(filters=16, kernel_size=21, strides=1, padding='same', input_shape=self.input_shape,\n",
    "                   kernel_initializer=keras.initializers.he_normal(), activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Conv1D(filters=32, kernel_size=11, strides=1, padding='same', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Conv1D(filters=64, kernel_size=5, strides=1, padding='same', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Flatten(),\n",
    "            Dense(2050, activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            Dropout(0.5),\n",
    "            Dense(self.nb_classes, activation='softmax', activity_regularizer=l1_l2(self.l1, self.wd))  # or Activation('softmax')\n",
    "        ])\n",
    "\n",
    "    def vgg9(self):\n",
    "        self.model = Sequential([\n",
    "            Conv1D(filters=64, kernel_size=21, strides=1, padding='same', activation='relu',\n",
    "                   input_shape=self.input_shape, kernel_initializer=keras.initializers.he_normal(), activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            Conv1D(filters=64, kernel_size=21, strides=1, padding='same', activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Conv1D(filters=128, kernel_size=11, strides=1, padding='same', activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            Conv1D(filters=128, kernel_size=11, strides=1, padding='same', activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Conv1D(filters=256, kernel_size=5, strides=1, padding='same', activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            Conv1D(filters=256, kernel_size=5, strides=1, padding='same', activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2, strides=2, padding='same'),\n",
    "            Flatten(),\n",
    "            Dense(4096, activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            Dropout(0.5),\n",
    "            Dense(4096, activation='relu', activity_regularizer=l1_l2(self.l1, self.wd)),\n",
    "            Dropout(0.5),\n",
    "            Dense(self.nb_classes, activation='softmax', activity_regularizer=l1_l2(self.l1, self.wd))  # or Activation('softmax')\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-giant",
   "metadata": {},
   "source": [
    "## Functions to compute MCC, sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "american-program",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# TODO use keras_confusion_matrix for everything\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def keras_confusion_matrix(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "#  matthews correlation coefficient\n",
    "def mcc(y_true, y_pred):\n",
    "    tp, tn, fp, fn = keras_confusion_matrix(y_true, y_pred)\n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    tp, _, fp, fn = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    tp, tn, fp, _ = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-harvard",
   "metadata": {},
   "source": [
    "## Save logs with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-journey",
   "metadata": {},
   "source": [
    "### Configuration of the hyperparameters and metrics to log with tensorboard  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-eleven",
   "metadata": {},
   "source": [
    "The following function is used to log the results with Tensorboard for each hyperparameters combinations in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "signal-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "class TensorboardLogging:\n",
    "\n",
    "    def __init__(self, hparams_filepath, params):\n",
    "        self.params = params\n",
    "        self.hparams_filepath = hparams_filepath\n",
    "        HP_EPOCHS = hp.HParam('epochs', hp.IntInterval(1, 50))\n",
    "        HP_LR = hp.HParam('lr', hp.RealInterval(1e-6, 1e-3))\n",
    "        HP_WD = hp.HParam('wd', hp.RealInterval(1e-8, 1e-3))\n",
    "        HP_BS = hp.HParam('bs', hp.IntInterval(1, 512))\n",
    "        HPARAMS = [HP_EPOCHS, HP_LR, HP_WD, HP_BS]\n",
    "        with tf.summary.create_file_writer(hparams_filepath).as_default():\n",
    "            hp.hparams_config(\n",
    "                hparams=HPARAMS,\n",
    "                metrics=[\n",
    "                    hp.Metric('train_accuracy', display_name='Train Accuracy'),\n",
    "                    hp.Metric('valid_accuracy', display_name='Valid Accuracy'),\n",
    "                    hp.Metric('test_accuracy', display_name='Test Accuracy'),\n",
    "                    hp.Metric('train_loss', display_name='Train Loss'),\n",
    "                    hp.Metric('valid_loss', display_name='Valid Loss'),\n",
    "                    hp.Metric('test_loss', display_name='Test Loss'),\n",
    "                    hp.Metric('train_mcc', display_name='Train MCC'),\n",
    "                    hp.Metric('valid_mcc', display_name='Valid MCC'),\n",
    "                    hp.Metric('test_mcc', display_name='Test MCC')\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def logging(self, traces):\n",
    "        epochs = self.params['n_epochs']\n",
    "        lr = self.params['lr']\n",
    "        wd = self.params['wd']\n",
    "        bs = self.params['bs']\n",
    "        l1 = self.params['l1']\n",
    "        with tf.summary.create_file_writer(self.hparams_filepath).as_default():\n",
    "            hp.hparams({\n",
    "                'epochs': epochs,\n",
    "                'lr': lr,\n",
    "                'wd': wd,\n",
    "                'bs': bs,\n",
    "                'l1': l1,\n",
    "            })  # record the values used in this trial\n",
    "            tf.summary.scalar('train_accuracy', np.mean([np.mean(x) for x in traces['train']['accuracies']]), step=1)\n",
    "            tf.summary.scalar('valid_accuracy', np.mean(traces['valid']['accuracies']), step=1)\n",
    "            tf.summary.scalar('test_accuracy', np.mean(traces['test']['accuracies']), step=1)\n",
    "            tf.summary.scalar('train_loss', np.mean([np.mean(x) for x in traces['train']['losses']]), step=1)\n",
    "            tf.summary.scalar('valid_loss', np.mean(traces['valid']['losses']), step=1)\n",
    "            tf.summary.scalar('test_loss', np.mean(traces['test']['losses']), step=1)\n",
    "            tf.summary.scalar('train_mcc', np.mean([np.mean(x) for x in traces['train']['mccs']]), step=1)\n",
    "            tf.summary.scalar('valid_mcc', np.mean(traces['valid']['mccs']), step=1)\n",
    "            tf.summary.scalar('test_mcc', np.mean(traces['test']['mccs']), step=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-cooling",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-hydrogen",
   "metadata": {},
   "source": [
    "First, let's get all the information on all individual biopsies contained in the dataset.\n",
    "<br>\n",
    "Each row represents a biopsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "built-engagement",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lab number</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>OCR number</th>\n",
       "      <th>Reception date</th>\n",
       "      <th>Tissue nature</th>\n",
       "      <th>Subtype</th>\n",
       "      <th>Pathological type</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Adicap code</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Age</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ds</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H15-3523</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Articular capsule of the knee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Myxosarcoma</td>\n",
       "      <td>Grade I</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Cavalier King charles</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Right back limb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ds</td>\n",
       "      <td>2.0</td>\n",
       "      <td>H15-3263</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Superficial dermis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fibrosarcoma</td>\n",
       "      <td>Grade III</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Cane Corso</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Perianal mass, invading the sub-cutaneous tissues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ds</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H15-2015</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Subcutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hemangiopericytoma</td>\n",
       "      <td>Grade I</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Golder Retriever</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ds</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H15-1570</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Subcutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MPNST</td>\n",
       "      <td>Grade II</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Crossbreed</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Left axillary region. \"Malignant Peripheral Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ds</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H15-1480</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Subcutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fibrosarcoma</td>\n",
       "      <td>Grade II</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Beagle</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Right back limb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ds</td>\n",
       "      <td>6.0</td>\n",
       "      <td>H15-1457</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Mucosal chorion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fibrosarcoma</td>\n",
       "      <td>Grade II</td>\n",
       "      <td>OH</td>\n",
       "      <td>OT</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Golder Retriever</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Labial mass, infiltrating the underneath muscu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ds</td>\n",
       "      <td>7.0</td>\n",
       "      <td>H15-1289</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fibrosarcoma</td>\n",
       "      <td>Grade II</td>\n",
       "      <td>OH</td>\n",
       "      <td>OT</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Golder Retriever</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Shoulder mass. Recurrence. May be a MPNST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ds</td>\n",
       "      <td>8.0</td>\n",
       "      <td>H15-1234</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Bone</td>\n",
       "      <td>Osteoblastic</td>\n",
       "      <td>Osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BH</td>\n",
       "      <td>LO</td>\n",
       "      <td>Q7A0</td>\n",
       "      <td>Bouledogue Français</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Mandibular bone. Infiltrating the mucosal chorion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ds</td>\n",
       "      <td>9.0</td>\n",
       "      <td>H15-1048</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Lung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indifferenciated</td>\n",
       "      <td>Grade III</td>\n",
       "      <td>OH</td>\n",
       "      <td>RP</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Shetland</td>\n",
       "      <td>8.0</td>\n",
       "      <td>90% Necrosis so difficult to diagnose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ds</td>\n",
       "      <td>10.0</td>\n",
       "      <td>H15-0037</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Mesenchymal tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fibrosarcoma</td>\n",
       "      <td>Grade II</td>\n",
       "      <td>OH</td>\n",
       "      <td>OT</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Berger Allemand</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Recurrence. Infiltrating the surrounding soft ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ds</td>\n",
       "      <td>11.0</td>\n",
       "      <td>H15-2397</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Subcutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indifferenciated</td>\n",
       "      <td>Grade III</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Setter</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Left anterior limb. Massively infiltrating. &gt;5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ds</td>\n",
       "      <td>12.0</td>\n",
       "      <td>H15-2364</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Subcutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rhabdomyosarcoma</td>\n",
       "      <td>Grade II</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Fox Terrier</td>\n",
       "      <td>15.0</td>\n",
       "      <td>No subtype determined by IHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ds</td>\n",
       "      <td>13.0</td>\n",
       "      <td>H15-1767</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Bone</td>\n",
       "      <td>Osteoblastic and chondroblastic</td>\n",
       "      <td>Osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>LO</td>\n",
       "      <td>Q7A0</td>\n",
       "      <td>Bouledogue Américain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One of the back limbs. Loci of cartilagoid matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ds</td>\n",
       "      <td>14.0</td>\n",
       "      <td>H15-1475</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Omentum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indifferenciated</td>\n",
       "      <td>Grade III</td>\n",
       "      <td>OH</td>\n",
       "      <td>XA</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Labrador</td>\n",
       "      <td>11.0</td>\n",
       "      <td>No subtype determined by IHC. &lt;50% Necrosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ds</td>\n",
       "      <td>15.0</td>\n",
       "      <td>H15-1184</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Bone</td>\n",
       "      <td>Osteoblastic</td>\n",
       "      <td>Osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>LO</td>\n",
       "      <td>Q7A0</td>\n",
       "      <td>Dogue Allemands</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Presence of osteoïd matrix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ds</td>\n",
       "      <td>16.0</td>\n",
       "      <td>H15-1130</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Bone</td>\n",
       "      <td>Osteoblastic</td>\n",
       "      <td>Osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>LO</td>\n",
       "      <td>Q7A0</td>\n",
       "      <td>Berger Belge Groenendael</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Costal bone. Same amount of cartilagoid and os...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ds</td>\n",
       "      <td>17.0</td>\n",
       "      <td>H15-1950</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Cutaneous and subcutaneous tissues</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hemangiopericytoma</td>\n",
       "      <td>Grade I</td>\n",
       "      <td>OH</td>\n",
       "      <td>TZ</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Braque Allemand</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Right buttock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ds</td>\n",
       "      <td>18.0</td>\n",
       "      <td>H15-1701</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>Bone</td>\n",
       "      <td>Osteoblastic</td>\n",
       "      <td>Osteosarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BH</td>\n",
       "      <td>LO</td>\n",
       "      <td>Q7A0</td>\n",
       "      <td>Dogue Argentin</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Moderate osteoid matrix production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ds</td>\n",
       "      <td>19.0</td>\n",
       "      <td>H14-0697</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>Spleen</td>\n",
       "      <td>Fibrohistiocytic</td>\n",
       "      <td>splenic sarcoma</td>\n",
       "      <td>Grade III</td>\n",
       "      <td>OH</td>\n",
       "      <td>SR</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Cocker</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Huge metastasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ds</td>\n",
       "      <td>20.0</td>\n",
       "      <td>H14-1242</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>Lung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Histiocytic sarcoma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>RP</td>\n",
       "      <td>X7K2</td>\n",
       "      <td>Bouvier Bernois</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ds</td>\n",
       "      <td>21.0</td>\n",
       "      <td>H15-1687</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>Mucosal chorion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fibrosarcoma</td>\n",
       "      <td>Grade II</td>\n",
       "      <td>OH</td>\n",
       "      <td>GG</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Rottweiler</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Vaginal mass, infiltrating, Necrosis and  hemo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ds</td>\n",
       "      <td>22.0</td>\n",
       "      <td>H16-0812</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>Cutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indifferenciated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>OT</td>\n",
       "      <td>X7K4</td>\n",
       "      <td>Border Collie</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This tumor could be a : Giant cell anaplastic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ds</td>\n",
       "      <td>23.0</td>\n",
       "      <td>H16-1124</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>Subcutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fibrosarcoma</td>\n",
       "      <td>Grade I</td>\n",
       "      <td>OH</td>\n",
       "      <td>OT</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Springer Anglais</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ds</td>\n",
       "      <td>24.0</td>\n",
       "      <td>H16-1397</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>Digestive tract</td>\n",
       "      <td>Spindle-shaped</td>\n",
       "      <td>GIST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OH</td>\n",
       "      <td>OT</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Bobtail</td>\n",
       "      <td>7.0</td>\n",
       "      <td>\"GastroIntestinal Stromal Tumor\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ds</td>\n",
       "      <td>25.0</td>\n",
       "      <td>H16-1707</td>\n",
       "      <td>2016-11-28</td>\n",
       "      <td>Subcutaneous tissue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Indifferenciated</td>\n",
       "      <td>Grade III</td>\n",
       "      <td>OH</td>\n",
       "      <td>OT</td>\n",
       "      <td>X7K0</td>\n",
       "      <td>Teckel</td>\n",
       "      <td>8.0</td>\n",
       "      <td>&gt;50% necrotic changes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ds</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ds</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ds</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ds</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ds</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ds</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ds</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Ds</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surgery : 20161202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Lab number  Unnamed: 1 OCR number Reception date  \\\n",
       "0          Ds         1.0   H15-3523     2016-05-12   \n",
       "1          Ds         2.0   H15-3263     2016-05-12   \n",
       "2          Ds         3.0   H15-2015     2016-05-12   \n",
       "3          Ds         4.0   H15-1570     2016-05-12   \n",
       "4          Ds         5.0   H15-1480     2016-05-12   \n",
       "5          Ds         6.0   H15-1457     2016-05-12   \n",
       "6          Ds         7.0   H15-1289     2016-05-12   \n",
       "7          Ds         8.0   H15-1234     2016-05-12   \n",
       "8          Ds         9.0   H15-1048     2016-05-12   \n",
       "9          Ds        10.0   H15-0037     2016-05-12   \n",
       "10         Ds        11.0   H15-2397     2016-05-12   \n",
       "11         Ds        12.0   H15-2364     2016-05-12   \n",
       "12         Ds        13.0   H15-1767     2016-05-12   \n",
       "13         Ds        14.0   H15-1475     2016-05-12   \n",
       "14         Ds        15.0   H15-1184     2016-05-12   \n",
       "15         Ds        16.0   H15-1130     2016-05-12   \n",
       "16         Ds        17.0   H15-1950     2016-05-12   \n",
       "17         Ds        18.0   H15-1701     2016-05-12   \n",
       "18         Ds        19.0   H14-0697     2016-11-28   \n",
       "19         Ds        20.0   H14-1242     2016-11-28   \n",
       "20         Ds        21.0   H15-1687     2016-11-28   \n",
       "21         Ds        22.0   H16-0812     2016-11-28   \n",
       "22         Ds        23.0   H16-1124     2016-11-28   \n",
       "23         Ds        24.0   H16-1397     2016-11-28   \n",
       "24         Ds        25.0   H16-1707     2016-11-28   \n",
       "25         Ds        26.0        NaN     2016-12-09   \n",
       "26         Ds        27.0        NaN     2016-12-09   \n",
       "27         Ds        28.0        NaN     2016-12-09   \n",
       "28         Ds        29.0        NaN     2016-12-09   \n",
       "29         Ds        30.0        NaN     2016-12-09   \n",
       "30         Ds        31.0        NaN     2016-12-09   \n",
       "31         Ds        32.0        NaN     2016-12-09   \n",
       "32         Ds        33.0        NaN     2016-12-09   \n",
       "\n",
       "                         Tissue nature                          Subtype  \\\n",
       "0        Articular capsule of the knee                              NaN   \n",
       "1                   Superficial dermis                              NaN   \n",
       "2                  Subcutaneous tissue                              NaN   \n",
       "3                  Subcutaneous tissue                              NaN   \n",
       "4                  Subcutaneous tissue                              NaN   \n",
       "5                      Mucosal chorion                              NaN   \n",
       "6                                  NaN                              NaN   \n",
       "7                                 Bone                     Osteoblastic   \n",
       "8                                 Lung                              NaN   \n",
       "9                   Mesenchymal tissue                              NaN   \n",
       "10                 Subcutaneous tissue                              NaN   \n",
       "11                 Subcutaneous tissue                              NaN   \n",
       "12                                Bone  Osteoblastic and chondroblastic   \n",
       "13                             Omentum                              NaN   \n",
       "14                                Bone                     Osteoblastic   \n",
       "15                                Bone                     Osteoblastic   \n",
       "16  Cutaneous and subcutaneous tissues                              NaN   \n",
       "17                                Bone                     Osteoblastic   \n",
       "18                              Spleen                 Fibrohistiocytic   \n",
       "19                                Lung                              NaN   \n",
       "20                     Mucosal chorion                              NaN   \n",
       "21                    Cutaneous tissue                              NaN   \n",
       "22                 Subcutaneous tissue                              NaN   \n",
       "23                     Digestive tract                   Spindle-shaped   \n",
       "24                 Subcutaneous tissue                              NaN   \n",
       "25                                 NaN                              NaN   \n",
       "26                                 NaN                              NaN   \n",
       "27                                 NaN                              NaN   \n",
       "28                                 NaN                              NaN   \n",
       "29                                 NaN                              NaN   \n",
       "30                                 NaN                              NaN   \n",
       "31                                 NaN                              NaN   \n",
       "32                                 NaN                              NaN   \n",
       "\n",
       "      Pathological type Classification Adicap code Unnamed: 9 Unnamed: 10  \\\n",
       "0           Myxosarcoma        Grade I          OH         TZ        X7K0   \n",
       "1          Fibrosarcoma      Grade III          OH         TZ        X7K0   \n",
       "2    Hemangiopericytoma        Grade I          OH         TZ        X7K0   \n",
       "3                 MPNST       Grade II          OH         TZ        X7K0   \n",
       "4          Fibrosarcoma       Grade II          OH         TZ        X7K0   \n",
       "5          Fibrosarcoma       Grade II          OH         OT        X7K0   \n",
       "6          Fibrosarcoma       Grade II          OH         OT        X7K0   \n",
       "7          Osteosarcoma            NaN          BH         LO        Q7A0   \n",
       "8      Indifferenciated      Grade III          OH         RP        X7K0   \n",
       "9          Fibrosarcoma       Grade II          OH         OT        X7K0   \n",
       "10     Indifferenciated      Grade III          OH         TZ        X7K0   \n",
       "11     Rhabdomyosarcoma       Grade II          OH         TZ        X7K0   \n",
       "12         Osteosarcoma            NaN          OH         LO        Q7A0   \n",
       "13     Indifferenciated      Grade III          OH         XA        X7K0   \n",
       "14         Osteosarcoma            NaN          OH         LO        Q7A0   \n",
       "15         Osteosarcoma            NaN          OH         LO        Q7A0   \n",
       "16   Hemangiopericytoma        Grade I          OH         TZ        X7K0   \n",
       "17         Osteosarcoma            NaN          BH         LO        Q7A0   \n",
       "18      splenic sarcoma      Grade III          OH         SR        X7K0   \n",
       "19  Histiocytic sarcoma            NaN          OH         RP        X7K2   \n",
       "20         Fibrosarcoma       Grade II          OH         GG        X7K0   \n",
       "21     Indifferenciated            NaN          OH         OT        X7K4   \n",
       "22         Fibrosarcoma        Grade I          OH         OT        X7K0   \n",
       "23                 GIST            NaN          OH         OT        X7K0   \n",
       "24     Indifferenciated      Grade III          OH         OT        X7K0   \n",
       "25               Normal            NaN         NaN        NaN         NaN   \n",
       "26               Normal            NaN         NaN        NaN         NaN   \n",
       "27               Normal            NaN         NaN        NaN         NaN   \n",
       "28               Normal            NaN         NaN        NaN         NaN   \n",
       "29               Normal            NaN         NaN        NaN         NaN   \n",
       "30               Normal            NaN         NaN        NaN         NaN   \n",
       "31               Normal            NaN         NaN        NaN         NaN   \n",
       "32               Normal            NaN         NaN        NaN         NaN   \n",
       "\n",
       "                       Breed   Age  \\\n",
       "0      Cavalier King charles   6.0   \n",
       "1                 Cane Corso   8.0   \n",
       "2           Golder Retriever  10.0   \n",
       "3                 Crossbreed  10.0   \n",
       "4                     Beagle  10.0   \n",
       "5           Golder Retriever   7.0   \n",
       "6           Golder Retriever  12.0   \n",
       "7        Bouledogue Français   9.0   \n",
       "8                   Shetland   8.0   \n",
       "9            Berger Allemand  12.0   \n",
       "10                    Setter  12.0   \n",
       "11               Fox Terrier  15.0   \n",
       "12      Bouledogue Américain   NaN   \n",
       "13                  Labrador  11.0   \n",
       "14           Dogue Allemands   4.0   \n",
       "15  Berger Belge Groenendael   2.0   \n",
       "16           Braque Allemand   9.0   \n",
       "17            Dogue Argentin   7.0   \n",
       "18                    Cocker  10.0   \n",
       "19           Bouvier Bernois  10.0   \n",
       "20                Rottweiler   9.0   \n",
       "21             Border Collie   5.0   \n",
       "22          Springer Anglais   5.0   \n",
       "23                   Bobtail   7.0   \n",
       "24                    Teckel   8.0   \n",
       "25                       NaN   NaN   \n",
       "26                       NaN   NaN   \n",
       "27                       NaN   NaN   \n",
       "28                       NaN   NaN   \n",
       "29                       NaN   NaN   \n",
       "30                       NaN   NaN   \n",
       "31                       NaN   NaN   \n",
       "32                       NaN   NaN   \n",
       "\n",
       "                                                Notes  \n",
       "0                                     Right back limb  \n",
       "1   Perianal mass, invading the sub-cutaneous tissues  \n",
       "2                                                 NaN  \n",
       "3   Left axillary region. \"Malignant Peripheral Ne...  \n",
       "4                                     Right back limb  \n",
       "5   Labial mass, infiltrating the underneath muscu...  \n",
       "6           Shoulder mass. Recurrence. May be a MPNST  \n",
       "7   Mandibular bone. Infiltrating the mucosal chorion  \n",
       "8               90% Necrosis so difficult to diagnose  \n",
       "9   Recurrence. Infiltrating the surrounding soft ...  \n",
       "10  Left anterior limb. Massively infiltrating. >5...  \n",
       "11                       No subtype determined by IHC  \n",
       "12  One of the back limbs. Loci of cartilagoid matrix  \n",
       "13        No subtype determined by IHC. <50% Necrosis  \n",
       "14                         Presence of osteoïd matrix  \n",
       "15  Costal bone. Same amount of cartilagoid and os...  \n",
       "16                                      Right buttock  \n",
       "17                 Moderate osteoid matrix production  \n",
       "18                                    Huge metastasis  \n",
       "19                                                NaN  \n",
       "20  Vaginal mass, infiltrating, Necrosis and  hemo...  \n",
       "21  This tumor could be a : Giant cell anaplastic ...  \n",
       "22                                                NaN  \n",
       "23                   \"GastroIntestinal Stromal Tumor\"  \n",
       "24                              >50% necrotic changes  \n",
       "25                                 Surgery : 20161201  \n",
       "26                                 Surgery : 20161128  \n",
       "27                                 Surgery : 20161201  \n",
       "28                                 Surgery : 20161129  \n",
       "29                                 Surgery : 20161206  \n",
       "30                                 Surgery : 20161129  \n",
       "31                                 Surgery : 20161201  \n",
       "32                                 Surgery : 20161202  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/mmc2.xlsx')\n",
    "df = df[df['Unnamed: 1'].notna()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southwest-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve the data from a csv file.\n",
    "def ms_data(fname):\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    mat_data = pd.read_csv(fname)\n",
    "    labels = mat_data.index.values\n",
    "    categories = [int(lab.split('_')[1]) for lab in labels]\n",
    "    labels = [lab.split('_')[0] for lab in labels]\n",
    "    mat_data = np.asarray(mat_data)\n",
    "    mat_data = minmax_scale(mat_data, axis=0, feature_range=(0, 1))\n",
    "    mat_data = mat_data.astype(\"float32\")\n",
    "    return mat_data, labels, categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "standing-clearing",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ms_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3a561872c2a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mms_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/canis_intensities.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ms_data' is not defined"
     ]
    }
   ],
   "source": [
    "data, labels, samples = ms_data('data/canis_intensities.csv')\n",
    "\n",
    "indices = np.argsort(samples)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "samples = samples[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "loose-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of spectra, including all duplicates\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-challenge",
   "metadata": {},
   "source": [
    "### List of all the samples and the number of biological duplicates for each sample.\n",
    "<br>\n",
    "There is a total of 2228 spectra which are from 33 biopsies. \n",
    "<br>\n",
    "Let's find out first how many duplicates we have for each individual biopsy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inner-temperature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Myxosarcoma_1': 60,\n",
       " 'Fibrosarcoma_10': 71,\n",
       " 'Indifferenciated_11': 78,\n",
       " 'Rhabdomyosarcoma_12': 66,\n",
       " 'Osteosarcoma_13': 69,\n",
       " 'Indifferenciated_14': 72,\n",
       " 'Osteosarcoma_15': 65,\n",
       " 'Osteosarcoma_16': 71,\n",
       " 'Hemangiopericytoma_17': 76,\n",
       " 'Osteosarcoma_18': 71,\n",
       " 'splenic sarcoma_19': 63,\n",
       " 'Fibrosarcoma_2': 65,\n",
       " 'Histiocytic sarcoma_20': 105,\n",
       " 'Fibrosarcoma_21': 64,\n",
       " 'Indifferenciated_22': 99,\n",
       " 'Fibrosarcoma_23': 69,\n",
       " 'GIST_24': 70,\n",
       " 'Indifferenciated_25': 60,\n",
       " 'Normal_26': 68,\n",
       " 'Normal_27': 73,\n",
       " 'Normal_28': 60,\n",
       " 'Normal_29': 60,\n",
       " 'Hemangiopericytoma_3': 58,\n",
       " 'Normal_30': 60,\n",
       " 'Normal_31': 46,\n",
       " 'Normal_32': 60,\n",
       " 'Normal_33': 55,\n",
       " 'MPNST_4': 60,\n",
       " 'Fibrosarcoma_5': 64,\n",
       " 'Fibrosarcoma_6': 62,\n",
       " 'Fibrosarcoma_7': 78,\n",
       " 'Osteosarcoma_8': 63,\n",
       " 'Indifferenciated_9': 67}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  TODO mettre ordre alphabetique\n",
    "samples_dict = {}\n",
    "for label, sample in zip(labels, samples):\n",
    "    name = f\"{label}_{sample}\"\n",
    "    if name not in samples_dict:\n",
    "        samples_dict[name] = 1\n",
    "    else:\n",
    "        samples_dict[name] += 1\n",
    "samples_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-indian",
   "metadata": {},
   "source": [
    "The samples names correspond to the pathology type of the sample. The number following the underscore is the sample's unique ID.\n",
    "\n",
    "<br><br>\n",
    "Note that some classes are only represented by a single biopsy. This is problematic, as we need to split the data into 3 subsets (train, valid and test sets). Thus, we will rename all biopsies that are not `Normal` as `Not Normal`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-tiger",
   "metadata": {},
   "source": [
    "Next, we'll retrieve the total number of spectra for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strong-quantum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Myxosarcoma': 60,\n",
       " 'Fibrosarcoma': 473,\n",
       " 'Indifferenciated': 376,\n",
       " 'Rhabdomyosarcoma': 66,\n",
       " 'Osteosarcoma': 339,\n",
       " 'Hemangiopericytoma': 134,\n",
       " 'splenic sarcoma': 63,\n",
       " 'Histiocytic sarcoma': 105,\n",
       " 'GIST': 70,\n",
       " 'Normal': 482,\n",
       " 'MPNST': 60}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = {}\n",
    "for label in labels:\n",
    "    if label not in labels_dict:\n",
    "        labels_dict[label] = 1\n",
    "    else:\n",
    "        labels_dict[label] += 1\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-aaron",
   "metadata": {},
   "source": [
    "For the reason stated above, all samples that are not `Normal` are relabeled `Not Normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "treated-camping",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'Normal' in labels:\n",
    "    for i, label in enumerate(labels):\n",
    "        if label != 'Normal':\n",
    "            labels[i] = 'Not Normal'\n",
    "categories = pd.Categorical(labels).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "scenic-harmony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Not Normal_1': 60,\n",
       " 'Not Normal_10': 71,\n",
       " 'Not Normal_11': 78,\n",
       " 'Not Normal_12': 66,\n",
       " 'Not Normal_13': 69,\n",
       " 'Not Normal_14': 72,\n",
       " 'Not Normal_15': 65,\n",
       " 'Not Normal_16': 71,\n",
       " 'Not Normal_17': 76,\n",
       " 'Not Normal_18': 71,\n",
       " 'Not Normal_19': 63,\n",
       " 'Not Normal_2': 65,\n",
       " 'Not Normal_20': 105,\n",
       " 'Not Normal_21': 64,\n",
       " 'Not Normal_22': 99,\n",
       " 'Not Normal_23': 69,\n",
       " 'Not Normal_24': 70,\n",
       " 'Not Normal_25': 60,\n",
       " 'Normal_26': 68,\n",
       " 'Normal_27': 73,\n",
       " 'Normal_28': 60,\n",
       " 'Normal_29': 60,\n",
       " 'Not Normal_3': 58,\n",
       " 'Normal_30': 60,\n",
       " 'Normal_31': 46,\n",
       " 'Normal_32': 60,\n",
       " 'Normal_33': 55,\n",
       " 'Not Normal_4': 60,\n",
       " 'Not Normal_5': 64,\n",
       " 'Not Normal_6': 62,\n",
       " 'Not Normal_7': 78,\n",
       " 'Not Normal_8': 63,\n",
       " 'Not Normal_9': 67}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: make sure everything is renamed properly\n",
    "samples_dict = {}\n",
    "for label, sample in zip(labels, samples):\n",
    "    name = f\"{label}_{sample}\"\n",
    "    if name not in samples_dict:\n",
    "        samples_dict[name] = 1\n",
    "    else:\n",
    "        samples_dict[name] += 1\n",
    "samples_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-kazakhstan",
   "metadata": {},
   "source": [
    "How many samples per class now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fewer-aluminum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Not Normal': 1746, 'Normal': 482}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = {}\n",
    "for label in labels:\n",
    "    if label not in labels_dict:\n",
    "        labels_dict[label] = 1\n",
    "    else:\n",
    "        labels_dict[label] += 1\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-korea",
   "metadata": {},
   "source": [
    "For convenience, we'll create a dataframe to store all pertinent information on all spectra.\n",
    "<br><br>\n",
    "The column `sample` contains the sample's ID (from 1 to 33)\n",
    "<br>\n",
    "The column `category` contains the new sample ID (0 or 1). If not binarised, the columns `sample` and `category` would contain exactly the same information.\n",
    "<br>\n",
    "The column `label` contains the name associated with the category. After binarisation, the category `0` is label `Normal` and `1` is label `Not Normal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "differential-settlement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Not Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2228 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample  category       label\n",
       "0          1         1  Not Normal\n",
       "1          1         1  Not Normal\n",
       "2          1         1  Not Normal\n",
       "3          1         1  Not Normal\n",
       "4          1         1  Not Normal\n",
       "...      ...       ...         ...\n",
       "2223       9         1  Not Normal\n",
       "2224       9         1  Not Normal\n",
       "2225       9         1  Not Normal\n",
       "2226       9         1  Not Normal\n",
       "2227       9         1  Not Normal\n",
       "\n",
       "[2228 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = pd.concat([\n",
    "    pd.DataFrame(np.array(samples).reshape([-1, 1])),\n",
    "    pd.DataFrame(np.array(categories).reshape([-1, 1])),\n",
    "    pd.DataFrame(np.array(labels).reshape([-1, 1])),\n",
    "], 1)\n",
    "labels_df.columns = ['sample', 'category', 'label']\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "figured-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will be used to split the samples ID-wise. \n",
    "# This function is only called once before the main loop for training with cross-validation.\n",
    "def split_train_test(labels_df):\n",
    "    from sklearn.model_selection import StratifiedKFold   \n",
    "    # First, get all unique samples and their category\n",
    "    unique_samples = []\n",
    "    unique_cats = []\n",
    "    for sample, cat in zip(labels_df['sample'], labels_df['category']):\n",
    "        if sample not in unique_samples:\n",
    "            unique_samples += [sample]\n",
    "            unique_cats += [cat]\n",
    "\n",
    "    # StratifiedKFold with n_splits of 5 to ranmdomly split 80/20.\n",
    "    # Used only once for train/test split.\n",
    "    # The train split needs to be split again into train/valid sets later\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    train_inds, test_inds = next(skf.split(unique_samples, unique_cats))\n",
    "    \n",
    "    # After the samples are split, we get the duplicates of all samples.\n",
    "    train_samples = [unique_samples[s] for s in train_inds]\n",
    "    test_samples = [unique_samples[s] for s in test_inds]\n",
    "    train_cats = [unique_cats[ind] for ind in train_inds]\n",
    "\n",
    "    return train_samples, test_samples, train_cats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-cream",
   "metadata": {},
   "source": [
    "The following class is used for everything related to training a Neural Network model. It is necessary in order to use the package `skopt`, which is used to learn hyperparameters using Bayesian optimization. In this case, we will use a Gaussian Process.\n",
    "\n",
    "Bayesian processes are sequential search strategies. They are usually faster at finding a local optimum, especially as the number of hyperparameters to optimise increase, because they use the results of previous runs to estimate which region in the hyperparemeter space has the most chance of improving the current optimimum.\n",
    "\n",
    "The following GIF might help to vizualise how guasssian processes work. As the iterations increase, it becomes clear where the next values of X have more potential of having higher gold content. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-tiger",
   "metadata": {},
   "source": [
    "<img src=\"gaussian_optimisation.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "steady-carroll",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(\n",
    "            self,\n",
    "            intensities_file,\n",
    "            cumulative_step,\n",
    "            criterion='categorical_crossentropy',\n",
    "            variant='logistic',\n",
    "            get_data_function=ms_data,\n",
    "            n_channels=1,\n",
    "            save_train_models=True,\n",
    "            model_name=CNN,\n",
    "            verbose=0,\n",
    "            model_path=False,\n",
    "            freeze=True,\n",
    "            retrain=True,\n",
    "    ):\n",
    "        self.dataset_name = intensities_file.split('/')[-1].split('_')[0]\n",
    "        self.freeze = freeze\n",
    "        self.verbose = verbose\n",
    "        self.variant = variant\n",
    "        self.retrain = retrain\n",
    "        self.cumulative_step = cumulative_step\n",
    "        self.model_name = model_name\n",
    "        self.save_train_models = save_train_models\n",
    "        self.data, labels, samples = get_data_function(intensities_file)\n",
    "        self.data[np.isnan(self.data)] = 0\n",
    "\n",
    "        # TODO have this step done in R import_data.R\n",
    "\n",
    "        if 'Normal' in labels:\n",
    "            for i, label in enumerate(labels):\n",
    "                if label != 'Normal':\n",
    "                    labels[i] = 'Not Normal'\n",
    "                    # categories[i] = 1\n",
    "                # else:\n",
    "                # categories[i] = 0\n",
    "        categories = pd.Categorical(labels).codes\n",
    "        self.labels_df = pd.concat([\n",
    "            pd.DataFrame(np.array(samples).reshape([-1, 1])),\n",
    "            pd.DataFrame(np.array(categories).reshape([-1, 1])),\n",
    "            pd.DataFrame(np.array(labels).reshape([-1, 1])),\n",
    "        ], 1)\n",
    "        self.labels_df.columns = ['sample', 'category', 'label']\n",
    "        self.nb_classes = len(np.unique(self.labels_df['category']))\n",
    "        self.input_shape = [self.data.shape[1], n_channels]\n",
    "        self.criterion = criterion\n",
    "        self.step = 0\n",
    "        self.call_num = 0\n",
    "        self.previous_datasets = \"\"\n",
    "        if model_path != 'None':\n",
    "            self.model_path, self.previous_datasets = self.select_best_model(model_path)\n",
    "        else:\n",
    "            self.model_path = False\n",
    "        if self.previous_datasets == \"\":\n",
    "            self.datasets = self.dataset_name\n",
    "        else:\n",
    "            self.datasets = f\"{self.previous_datasets}_{self.dataset_name}\"\n",
    "        self.params = {}\n",
    "\n",
    "    # This function of class Train is not used to train the first model and is ommitted to simplify the code. \n",
    "    # Implemented and used notebook train_second_model (long).ipynb\n",
    "    def select_best_model(self, params_fname):\n",
    "        pass\n",
    "    \n",
    "    # This function of class Train is not used to train the first model and is ommitted to simplify the code. \n",
    "    # Implemented and used notebook train_second_model (long).ipynb\n",
    "    def update_model(self, model_source, path, wd):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def train(self, h_params):\n",
    "        self.call_num += 1\n",
    "        n_epochs = h_params[0]\n",
    "        lr = h_params[1]\n",
    "        wd = h_params[2]\n",
    "        l1 = h_params[3]\n",
    "        bs = h_params[4]\n",
    "\n",
    "        h_params = {\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"lr\": lr,\n",
    "            \"wd\": wd,\n",
    "            \"l1\": l1,\n",
    "            \"bs\": bs,\n",
    "        }\n",
    "\n",
    "        path = \\\n",
    "            f'{self.criterion}/' + \\\n",
    "            f'{n_epochs}/' \\\n",
    "            f'{\"{:.8f}\".format(float(lr))}/' \\\n",
    "            f'{\"{:.8f}\".format(float(wd))}/' \\\n",
    "            f'{\"{:.8f}\".format(float(l1))}/' \\\n",
    "            f'{bs}/'\n",
    "        \n",
    "        hparams_filepath = f\"logs_notebook/CNN/{self.variant}/{self.datasets}/{self.freeze}/{self.retrain}\" \\\n",
    "                           f\"/hparam_tuning/{path}\"\n",
    "        log_filepath = f\"logs_notebook/CNN/{self.variant}/{self.datasets}/{self.freeze}/{self.retrain}/{path}\"\n",
    "\n",
    "        os.makedirs(log_filepath, exist_ok=True)\n",
    "        tb_logging = TensorboardLogging(hparams_filepath, h_params)\n",
    "        traces = {\n",
    "            \"train\": {\n",
    "                \"losses\": [],\n",
    "                \"accuracies\": [],\n",
    "                \"mccs\": [],\n",
    "                \"sensitivities\": [],\n",
    "                \"specificities\": [],\n",
    "            },\n",
    "            \"valid\": {\n",
    "                \"losses\": [],\n",
    "                \"accuracies\": [],\n",
    "                \"mccs\": [],\n",
    "                \"sensitivities\": [],\n",
    "                \"specificities\": [],\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"losses\": [],\n",
    "                \"accuracies\": [],\n",
    "                \"mccs\": [],\n",
    "                \"sensitivities\": [],\n",
    "                \"specificities\": [],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        ##############################################################\n",
    "        # Samples are split ID-wise\n",
    "        # All replicates of a sample need to be included in the same\n",
    "        # set (train/valid/test)\n",
    "        ##############################################################\n",
    "        \n",
    "        all_train_samples, test_samples, train_cats = split_train_test(self.labels_df)\n",
    "        all_train_indices = [s for s, lab in enumerate(self.labels_df['sample']) if lab in all_train_samples]\n",
    "        test_indices = [s for s, lab in enumerate(self.labels_df['sample']) if lab in test_samples]\n",
    "\n",
    "        x_test = self.data[test_indices]\n",
    "        y_test = self.labels_df['category'][test_indices].tolist()\n",
    "\n",
    "        assert len(set(y_test)) == self.nb_classes\n",
    "\n",
    "        # 3-fold CV\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "        for i, (train_samples, valid_samples) in enumerate(skf.split(all_train_samples, train_cats)):\n",
    "            # Just plot the first iteration, it will already be crowded if doing > 100 optimization iterations\n",
    "            if self.verbose:\n",
    "                print(f\"CV: {i}\")\n",
    "\n",
    "            # Now valid_samples and train_samples are indices of all_train_samples.\n",
    "            # So we first need to get the correct samples indices\n",
    "            valid_samples = [all_train_samples[s] for s in valid_samples]\n",
    "            train_samples = [all_train_samples[s] for s in train_samples]\n",
    "            \n",
    "            \n",
    "            # Next, we get all the indices for all replicates of each set\n",
    "            train_indices = [s for s, lab in enumerate(self.labels_df['sample'].tolist()) if lab in train_samples]\n",
    "            valid_indices = [s for s, lab in enumerate(self.labels_df['sample'].tolist()) if lab in valid_samples]\n",
    "\n",
    "            x_train = self.data[train_indices]\n",
    "            y_train = self.labels_df['category'][train_indices]\n",
    "            x_valid = self.data[valid_indices]\n",
    "            y_valid = self.labels_df['category'][valid_indices]\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(x_train)\n",
    "            x_train = scaler.transform(x_train)\n",
    "            x_valid = scaler.transform(x_valid)\n",
    "\n",
    "            x_train_conv = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "            x_valid_conv = np.reshape(x_valid, (x_valid.shape[0], x_valid.shape[1], 1))\n",
    "            y_train_conv = to_categorical(y_train, self.nb_classes)\n",
    "            y_valid_conv = to_categorical(y_valid, self.nb_classes)\n",
    "\n",
    "            dnn = self.model_name(h_params, self.nb_classes, batch_size=bs, variant=self.variant, activation='relu')\n",
    "            dnn.build(input_shape=self.input_shape)\n",
    "\n",
    "            if self.model_path:\n",
    "                dnn = self.update_model(model_source=dnn, path=self.model_path, wd=wd)\n",
    "            dnn.model.compile(loss=self.criterion,\n",
    "                              optimizer='adam',\n",
    "                              metrics=['accuracy', mcc])\n",
    "            callbacks = []\n",
    "            if i == 0:\n",
    "                callbacks += [keras.callbacks.TensorBoard(\n",
    "                    log_dir=log_filepath,\n",
    "                    histogram_freq=1,\n",
    "                    write_graph=True,\n",
    "                    write_images=False,\n",
    "                    update_freq=\"epoch\",\n",
    "                    profile_batch=2,\n",
    "                    embeddings_freq=0,\n",
    "                    embeddings_metadata=None,\n",
    "                )]\n",
    "            callbacks += [keras.callbacks.EarlyStopping(\n",
    "                monitor='loss',\n",
    "                min_delta=0,\n",
    "                patience=10,\n",
    "                verbose=self.verbose,\n",
    "                mode='min'\n",
    "            )]\n",
    "\n",
    "            y_integers = np.argmax(y_train_conv, axis=1)\n",
    "            class_weights = compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)\n",
    "            d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "            # CAN't USE VALIDATION SPLIT, so also can't use early_stopping or reduceLROnPLateau\n",
    "            if self.verbose == 2:\n",
    "                fit_verbose = 1\n",
    "            else:\n",
    "                fit_verbose = 0\n",
    "            history = dnn.model.fit(\n",
    "                x=x_train_conv,\n",
    "                y=y_train_conv,\n",
    "                batch_size=bs,\n",
    "                verbose=fit_verbose,\n",
    "                epochs=n_epochs,\n",
    "                validation_split=0.,\n",
    "                class_weight=d_class_weights,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            base_path = f'saved_models/keras/CNN/{self.variant}/{self.datasets}/' \\\n",
    "                        f'{self.freeze}/{self.retrain}/'\n",
    "            file_path = f'{base_path}/{path}'\n",
    "            os.makedirs(file_path, exist_ok=True)\n",
    "\n",
    "            train_acc = history.history['accuracy']\n",
    "            train_loss = history.history['loss']\n",
    "            train_mcc = history.history['mcc']\n",
    "\n",
    "            # train_loss, train_acc, train_mcc = dnn.model.evaluate(x_train_conv, y_train_conv, verbose=self.verbose)\n",
    "            y_classes_train = np.argmax(dnn.model.predict(x_train_conv), axis=-1)\n",
    "            train_sensitivity, train_specificity = compute_confusion_matrix(y_train, y_classes_train)\n",
    "\n",
    "            valid_loss, valid_acc, valid_mcc = dnn.model.evaluate(x_valid_conv, y_valid_conv, verbose=self.verbose)\n",
    "            y_classes_valid = np.argmax(dnn.model.predict(x_valid_conv), axis=-1)\n",
    "            valid_sensitivity, valid_specificity = compute_confusion_matrix(y_valid, y_classes_valid)\n",
    "\n",
    "            best_epoch = np.argmin(valid_loss)\n",
    "\n",
    "            traces['train']['losses'].append(train_loss)\n",
    "            traces['train']['accuracies'].append(train_acc)\n",
    "            traces['train']['mccs'].append(train_mcc)\n",
    "            traces['train']['sensitivities'].append(train_sensitivity)\n",
    "            traces['train']['specificities'].append(train_specificity)\n",
    "            traces['valid']['losses'].append(valid_loss)\n",
    "            traces['valid']['accuracies'].append(valid_acc)\n",
    "            traces['valid']['mccs'].append(valid_mcc)\n",
    "            traces['valid']['sensitivities'].append(valid_sensitivity)\n",
    "            traces['valid']['specificities'].append(valid_specificity)\n",
    "\n",
    "        if self.retrain:\n",
    "            x_all_train = self.data[all_train_indices]\n",
    "            y_all_train = self.labels_df['category'][all_train_indices]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(x_all_train)\n",
    "            all_x_train = scaler.transform(x_all_train)\n",
    "\n",
    "            x_train_conv = np.reshape(all_x_train, (all_x_train.shape[0], all_x_train.shape[1], 1))\n",
    "            y_train_conv = to_categorical(y_all_train, self.nb_classes)\n",
    "\n",
    "            history = dnn.model.fit(\n",
    "                x=x_train_conv,\n",
    "                y=y_train_conv,\n",
    "                batch_size=bs,\n",
    "                verbose=fit_verbose,\n",
    "                epochs=n_epochs,\n",
    "                validation_split=0.,\n",
    "                class_weight=d_class_weights,\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "\n",
    "        x_test_trans = scaler.transform(x_test)\n",
    "        x_test_conv = np.reshape(x_test_trans, (x_test_trans.shape[0], x_test_trans.shape[1], 1))\n",
    "        y_test_conv = to_categorical(y_test, self.nb_classes)\n",
    "\n",
    "        test_loss, test_acc, test_mcc = dnn.model.evaluate(x_test_conv, y_test_conv, verbose=self.verbose)\n",
    "        y_classes_test = np.argmax(dnn.model.predict(x_test_conv), axis=-1)\n",
    "        test_sensitivity, test_specificity = compute_confusion_matrix(y_test, y_classes_test)\n",
    "        traces['test']['losses'].append(test_loss)\n",
    "        traces['test']['accuracies'].append(test_acc)\n",
    "        traces['test']['mccs'].append(test_mcc)\n",
    "        traces['test']['specificities'].append(test_specificity)\n",
    "        traces['test']['sensitivities'].append(test_sensitivity)\n",
    "        # dnn.model.summary()\n",
    "\n",
    "        dnn.model.save_weights(f'{file_path}/{self.variant}_{self.cumulative_step}.h5')\n",
    "        self.step += 1\n",
    "        try:\n",
    "            tb_logging.logging(traces)\n",
    "        except:\n",
    "            print(\"\\nProblem with logging\\n\")\n",
    "\n",
    "        self.params[f\"call_{self.call_num}\"] = {\n",
    "            'datasets': self.datasets,\n",
    "            'h_params': {\n",
    "                'criterion': f'{self.criterion}',\n",
    "                'n_epochs': f'{n_epochs}',\n",
    "                'lr': f'{lr}',\n",
    "                'wd': f'{wd}',\n",
    "                'l1': f'{l1}',\n",
    "                'bs': f'{bs}',\n",
    "            },\n",
    "            'scores': {\n",
    "                'best_epoch': f'{best_epoch}',\n",
    "                'train_loss': f'{train_loss[int(best_epoch)]}',\n",
    "                'valid_loss': f'{valid_loss}',\n",
    "                'test_loss': f'{test_loss}',\n",
    "                'train_acc': f'{train_acc[int(best_epoch)]}',\n",
    "                'valid_acc': f'{valid_acc}',\n",
    "                'test_acc': f'{test_acc}',\n",
    "                'train_mcc': f'{train_mcc[int(best_epoch)]}',\n",
    "                'valid_mcc': f'{valid_mcc}',\n",
    "                'test_mcc': f'{test_mcc}',\n",
    "            }\n",
    "        }\n",
    "        json.dump(self.params, open(f'{base_path}/params.json', 'w'))\n",
    "\n",
    "        return 1 - np.mean(traces['valid']['accuracies'])\n",
    "\n",
    "    def test(self, params):\n",
    "        pass\n",
    "\n",
    "def compute_confusion_matrix(y_test, y_classes):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_classes).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    return sensitivity, specificity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-dodge",
   "metadata": {},
   "source": [
    "Use the following cell to launch tensorboard directly in the notebook. <br>\n",
    "If you have trouble lauching the tensorboard from the notebook, try launching it from a terminal with the command:<br>\n",
    "`tensorboard --logdir logs_notebook` <br>\n",
    "It is easier to use a dedicated tab to see the results anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "needed-reggae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 35396), started 4:28:57 ago. (Use '!kill 35396' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-23b8c1e9392456de\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-23b8c1e9392456de\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs_notebook/CNN/lecun/canis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "alive-moscow",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV: 0\n",
      "21/21 [==============================] - 1s 12ms/step - loss: 10.9299 - accuracy: 0.8779 - mcc: 0.7619\n",
      "CV: 1\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 5.6473 - accuracy: 0.7972 - mcc: 0.6042\n",
      "CV: 2\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.9889 - accuracy: 0.7066 - mcc: 0.4176\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.0930 - accuracy: 0.8569 - mcc: 0.7094\n",
      "CV: 0\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 23.4548 - accuracy: 0.8275 - mcc: 0.6637\n",
      "CV: 1\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7574 - accuracy: 0.7829 - mcc: 0.5656\n",
      "CV: 2\n",
      "16/16 [==============================] - 1s 8ms/step - loss: 18.2461 - accuracy: 0.7625 - mcc: 0.5352\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 40.5207 - accuracy: 0.7431 - mcc: 0.4862\n",
      "CV: 0\n",
      "21/21 [==============================] - 1s 8ms/step - loss: 250.7449 - accuracy: 0.8031 - mcc: 0.6161\n",
      "CV: 1\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 35.8131 - accuracy: 0.8203 - mcc: 0.6493\n",
      "CV: 2\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 43.3045 - accuracy: 0.8463 - mcc: 0.6972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-d456cd965d25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m ]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mtest_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mtest_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\AirSim\\lib\\site-packages\\skopt\\optimizer\\gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    257\u001b[0m             noise=noise)\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m     return base_minimize(\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0macq_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macq_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\AirSim\\lib\\site-packages\\skopt\\optimizer\\base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mnext_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-167874ec9487>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, h_params)\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[0my_train_conv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_all_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m             history = dnn.model.fit(\n\u001b[0m\u001b[0;32m    261\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train_conv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_conv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1190\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 867\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    513\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \"\"\"\n\u001b[0;32m   1093\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = Train(intensities_file='data/canis_intensities.csv',\n",
    "              cumulative_step=0,\n",
    "              criterion= 'categorical_crossentropy',\n",
    "              variant='lecun',\n",
    "              verbose=1,\n",
    "              model_name=CNN,\n",
    "              freeze=1,\n",
    "              retrain=1\n",
    "              )\n",
    "\n",
    "os.makedirs(f'logs_notebook/hparam_tuning', exist_ok=True)\n",
    "\n",
    "from skopt.space import Real, Integer\n",
    "from skopt import gp_minimize\n",
    "\n",
    "space = [\n",
    "    Integer(1, 50, \"uniform\", name='epochs'),\n",
    "    Real(1e-6, 1e-3, \"log-uniform\", name='lr'),\n",
    "    Real(1e-8, 1e-3, \"log-uniform\", name='wd'),\n",
    "    Real(1e-8, 1e-3, \"log-uniform\", name='l1'),\n",
    "    Integer(1, 512, \"uniform\", name='bs'),\n",
    "]\n",
    "\n",
    "test_mean = gp_minimize(train.train, space, n_calls=100, random_state=42)\n",
    "test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(params_fname):\n",
    "    # import from json file\n",
    "    params = pd.read_json(params_fname)\n",
    "    best_call = 0\n",
    "    best_valid_loss = np.inf\n",
    "    for call in list(params.keys()):\n",
    "        values = params[call]\n",
    "        if float(values['scores']['valid_loss']) < best_valid_loss:\n",
    "            best_valid_loss = float(values['scores']['valid_loss'])\n",
    "            best_call = call\n",
    "    hparams = params[best_call]['h_params']\n",
    "\n",
    "    return hparams\n",
    "\n",
    "# True and True for freezing the weights and retrain after cross-validation.\n",
    "# TODO save with meaningful folder names\n",
    "select_best_model(\"saved_models/keras/CNN/lecun/canis/True/True/params.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-maker",
   "metadata": {},
   "source": [
    "To see the results, use tensorboard or (not recommended) change the verbose option to 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
